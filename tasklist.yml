version: 1
generated_at: 2025-09-06T00:00:00Z
owner: platform/dirsrv
tasks:
  - title: "Compose: Use dsnet network and FQDN hostnames (s1/s2/c1/c2)"
    status: fixed
    priority: P0
    prompt: |
      Update compose to match the design’s deterministic networking and naming:
      - Rename the user-defined network to `dsnet`.
      - Set container hostnames to FQDNs: `s1.dsnet.test`, `s2.dsnet.test`, `c1.dsnet.test`, `c2.dsnet.test`.
      - Add DNS aliases for both short and FQDN names on the `dsnet` network.
      - Optionally rename containers to short names (`s1`, `s2`, `c1`, `c2`) for easier `podman exec`.
      - Keep host ports unexposed by default (access is container-to-container on dsnet).
    files_to_edit:
      - compose/podman-compose.389ds.yml
    snippets:
      - path: compose/podman-compose.389ds.yml
        language: yaml
        content: |
          networks:
            dsnet:
              driver: bridge
              name: dsnet

          services:
            s1:
              image: quay.io/389ds/dirsrv:latest
              container_name: s1
              hostname: s1.dsnet.test
              networks:
                dsnet:
                  aliases: ["s1", "s1.dsnet.test"]
              # ... env/healthcheck/volumes (see other tasks)
            # repeat similarly for s2, c1, c2
    commands:
      - podman network rm dsnet || true
      - podman network create dsnet
    acceptance_criteria:
      - `podman network inspect dsnet` exists and containers attach to it.
      - From any container, `getent hosts s1.dsnet.test` resolves to the peer.
      - From `s2`, `ldapsearch -x -H ldap://s1.dsnet.test:389 -s base -b '' 1.1` succeeds after startup.
    notes: |
      Using FQDNs enables TLS SAN matching later. Do not attach containers to multiple user networks.

  - title: "Compose: Standardize internal LDAP port to 389 and remove DS_PORT=3389"
    status: fixed
    priority: P0
    prompt: |
      Align the container’s LDAP port to the default 389 (design uses 389/636):
      - Remove `DS_PORT=3389` from all services.
      - If setting explicitly, use `DS_PORT=389`.
      - Update any healthchecks and test files that reference 3389 to use 389.
    files_to_edit:
      - compose/podman-compose.389ds.yml
      - test/repl_vars.yml
      - test/repl_mesh_vars.yml
      - test/repl_mesh.yml
      - Makefile (any 3389 occurrences)
    acceptance_criteria:
      - Healthcheck uses 389; `podman exec s1 ss -ltn` shows 389.
      - All references to 3389 are removed or updated to 389.

  - title: "Compose: Add explicit volumes for config/data/certs/logs with :Z"
    status: fixed
    priority: P1
    prompt: |
      Persist DS data and prepare for TLS by mounting explicit volumes:
      - For each service, bind-mount:
        - `/etc/dirsrv/slapd-<inst>` (config, includes NSS DB)
        - `/var/lib/dirsrv/slapd-<inst>` (data)
        - `/etc/dirsrv/slapd-<inst>/certs` (NSS DB subdir; explicit for clarity)
        - `/var/log/dirsrv/slapd-<inst>` (logs)
      - Use `:Z` suffix for SELinux label (harmless on macOS but good practice).
      - Choose host paths; for design parity prefer a VM path like `/srv/389ds-lab/...` or keep current `.ansible/containers/...` for tests.
    files_to_edit:
      - compose/podman-compose.389ds.yml
    snippets:
      - path: compose/podman-compose.389ds.yml
        language: yaml
        content: |
          services:
            s1:
              volumes:
                - type: bind
                  source: ../.ansible/containers/s1/etc-dirsrv-slapd-s1
                  target: /etc/dirsrv/slapd-s1:Z
                - type: bind
                  source: ../.ansible/containers/s1/var-lib-dirsrv-slapd-s1
                  target: /var/lib/dirsrv/slapd-s1:Z
                - type: bind
                  source: ../.ansible/containers/s1/etc-dirsrv-slapd-s1-certs
                  target: /etc/dirsrv/slapd-s1/certs:Z
                - type: bind
                  source: ../.ansible/containers/s1/var-log-dirsrv-slapd-s1
                  target: /var/log/dirsrv/slapd-s1:Z
            # replicate for s2,c1,c2 with matching instance names
    acceptance_criteria:
      - DS logs persist across restarts under host path; configs survive `podman compose down/up`.

  - title: "Compose: Switch healthchecks to LDAPI (EXTERNAL bind)"
    status: fixed
    priority: P1
    prompt: |
      Use a robust readiness probe that does not depend on TCP or credentials:
      - Replace healthcheck `test` with `ldapsearch -Y EXTERNAL -H ldapi://%2Fdata%2Frun%2Fslapd-localhost.socket -s base -b '' 1.1`.
      - Use `interval: 5s`, `retries: 50`, `timeout: 2s` for stability.
    files_to_edit:
      - compose/podman-compose.389ds.yml
    snippets:
      - path: compose/podman-compose.389ds.yml
        language: yaml
        content: |
          healthcheck:
            test: ["CMD-SHELL", "ldapsearch -Y EXTERNAL -H ldapi://%2Fdata%2Frun%2Fslapd-localhost.socket -s base -b '' 1.1 >/dev/null || exit 1"]
            interval: 5s
            timeout: 2s
            retries: 50
    acceptance_criteria:
      - `podman ps` shows containers becoming healthy after DS is up.

  - title: "Compose: Use DS_SUFFIX_NAME per design; remove DS_SUFFIX"
    status: fixed
    priority: P1
    prompt: |
      Align instance initialization variables with the design and upstream image support:
      - Replace `DS_SUFFIX=o=example` with `DS_SUFFIX_NAME=dc=example,dc=com`.
      - Ensure Directory Manager password env remains set (`DS_DM_PASSWORD=password`).
    files_to_edit:
      - compose/podman-compose.389ds.yml
    acceptance_criteria:
      - First startup creates suffix `dc=example,dc=com` without manual dsconf calls.

  - title: "Inventory: Add inventories/lab/hosts.yml with FQDNs and replica IDs"
    status: fixed
    priority: P0
    prompt: |
      Create a lab inventory that matches the design naming and replication roles:
      - Suppliers: `s1.dsnet.test` (replica_id 1001), `s2.dsnet.test` (replica_id 1002)
      - Consumers: `c1.dsnet.test` (replica_id 2001), `c2.dsnet.test` (replica_id 2002)
      - Set `ansible_host` to FQDN; when using the Podman connection plugin keep the inventory hostnames as container names but set separate vars for replication host/port (see Replication task).
    files_to_add:
      - inventories/lab/hosts.yml
    snippets:
      - path: inventories/lab/hosts.yml
        language: yaml
        content: |
          all:
            children:
              suppliers:
                hosts:
                  s1.dsnet.test:
                    replica_id: 1001
                    ldap_port: 389
                    ldaps_port: 636
                    instance_name: "localhost"
                  s2.dsnet.test:
                    replica_id: 1002
                    ldap_port: 389
                    ldaps_port: 636
                    instance_name: "localhost"
              consumers:
                hosts:
                  c1.dsnet.test:
                    replica_id: 2001
                    ldap_port: 389
                    ldaps_port: 636
                    instance_name: "localhost"
                  c2.dsnet.test:
                    replica_id: 2002
                    ldap_port: 389
                    ldaps_port: 636
                    instance_name: "localhost"
    acceptance_criteria:
      - `ansible-inventory -i inventories/lab/hosts.yml --graph` shows suppliers/consumers with four hosts.

  - title: "Group Vars: Normalize suffix to dc=example,dc=com"
    status: fixed
    priority: P0
    prompt: |
      Unify the replicated suffix across the repo to the design default `dc=example,dc=com`:
      - Update `group_vars/all/dirsrv.yml` to set `dirsrv_default_suffix: dc=example,dc=com`.
      - Update test vars (e.g., `test/compose_vars.yml`, `test/seed.yml`) replacing `o=example` with `dc=example,dc=com`.
      - Update compose env (`DS_SUFFIX_NAME`) if present.
    files_to_edit:
      - group_vars/all/dirsrv.yml
      - test/compose_vars.yml
      - test/seed.yml
      - compose/podman-compose.389ds.yml
    acceptance_criteria:
      - All plays and roles operate on `dc=example,dc=com`; no stray `o=example` remain.

  - title: "TLS: Implement dirs389.tls role (CA trust, server cert import, LDAPS enable)"
    status: fixed
    priority: P0
    prompt: |
      Create a new role `roles/dirs389.tls` (or extend an existing TLS role) to:
      1) Import lab CA into each instance's NSS DB (trust flags CT,C,C).
      2) Import per-host server certificate and key; set nickname equal to FQDN (e.g., `s1.dsnet.test`).
      3) Enable TLS by configuring:
         - `nsslapd-securePort=636`
         - `nsSSLPersonalitySSL="<FQDN-nickname>"`
      4) Restart the instance (dsctl <inst> restart).
      Use LDAPI with `-Y EXTERNAL` where possible. Inputs can be CSR artifacts signed by a local CA or prebuilt test certs.
    files_to_add:
      - roles/dirs389.tls/tasks/main.yml
    snippets:
      - path: roles/dirs389.tls/tasks/main.yml
        language: yaml
        content: |
          ---
          - name: Import CA certificate (trust CT,C,C)
            ansible.builtin.command:
              argv: [certutil, -A, -d, "/etc/dirsrv/slapd-{{ dirsrv_instance | default('localhost') }}", -n, "lab-CA", -t, "CT,C,C", -a, -i, "{{ dirsrv_tls_ca_crt }}"]
            register: add_ca
            changed_when: add_ca.rc == 0
            failed_when: false

          - name: Import server certificate (primary)
            ansible.builtin.command:
              argv: [dsconf, "{{ dirsrv_instance | default('localhost') }}", security, certificate, add, --file, "{{ dirsrv_tls_server_crt }}", --name, "{{ inventory_hostname }}", --primary-cert]
            register: add_srv
            changed_when: add_srv.rc == 0
            failed_when: false

          - name: Enable secure port 636
            ansible.builtin.command:
              argv: [dsconf, "{{ dirsrv_instance | default('localhost') }}", config, replace, "nsslapd-securePort=636"]
            register: sec_port
            changed_when: sec_port.rc == 0
            failed_when: false

          - name: Set certificate nickname (nsSSLPersonalitySSL)
            ansible.builtin.command:
              argv: [dsconf, "{{ dirsrv_instance | default('localhost') }}", config, replace, "nsSSLPersonalitySSL={{ inventory_hostname }}"]
            register: set_nick
            changed_when: set_nick.rc == 0
            failed_when: false

          - name: Restart instance to apply TLS
            ansible.builtin.command:
              argv: [dsctl, "{{ dirsrv_instance | default('localhost') }}", restart]
            register: restart
            changed_when: restart.rc == 0
            failed_when: false
    acceptance_criteria:
      - `dsconf <inst> get-config | grep -E "nsslapd-securePort|nsSSLPersonalitySSL"` shows expected values.
      - From peer container, `openssl s_client -connect s1.dsnet.test:636 -CAfile <lab-ca.crt>` returns `Verify return code: 0 (ok)`.

  - title: "Replication: Switch nodes to LDAPS and FQDNs; ensure unique supplier IDs"
    status: fixed
    priority: P0
    prompt: |
      Update replication variables to use FQDNs over LDAPS 636 and verify replica IDs:
      - In `test/repl_vars.yml` and `test/repl_mesh_vars.yml`, set `protocol: LDAPS`, `port: 636`, and `host: <FQDN>` for each node.
      - Ensure `dirsrv_repl_replica_ids` assigns unique IDs to all suppliers/hubs for each suffix.
      - Keep inventory hostnames for Podman plugin as-is; only the `host` field should be the FQDN used by agreements.
    files_to_edit:
      - test/repl_vars.yml
      - test/repl_mesh_vars.yml
    snippets:
      - path: test/repl_mesh_vars.yml
        language: yaml
        content: |
          dirsrv_repl_nodes:
            ds-s1: { role: supplier, instance: "localhost", host: "s1.dsnet.test", port: 636, protocol: LDAPS }
            ds-c1: { role: consumer, instance: "localhost", host: "c1.dsnet.test", port: 636, protocol: LDAPS }
            ds-s2: { role: supplier, instance: "localhost", host: "s2.dsnet.test", port: 636, protocol: LDAPS }
            ds-c2: { role: consumer, instance: "localhost", host: "c2.dsnet.test", port: 636, protocol: LDAPS }
    acceptance_criteria:
      - Agreements list (`dsconf ... repl-agmt list --suffix ...`) shows hosts `*.dsnet.test:636`.

  - title: "Replication: Resolve generation ID mismatches by clean re-init"
    status: todo
    priority: P0
    prompt: |
      Address generation ID mismatches seen in logs (e.g., s2->c2):
      - Run `make reset_soft` to restore golden backups (or `make reset_hard` to fully rebuild).
      - Ensure only one `init: true` per link (supplier→consumer) in vars; no bidirectional init for same pair.
      - Re-run mesh play and wait for green with the role’s `wait_green`.
      - If needed, manually run `dsconf ... repl-agmt init --suffix dc=example,dc=com <agmt-name>` on supplier, then monitor `repl-agmt status` until `"error": 0` and not `in progress`.
    files_to_edit:
      - test/repl_mesh_vars.yml
    commands:
      - make reset_soft
      - make test_repl_mesh ARGS="-e dirsrv_debug=true -e dirsrv_log_capture=true -vvv"
    acceptance_criteria:
      - No WARN lines about generation ID mismatch in `podman-ds-*.log`.
      - `wait_green` completes without failure.

  - title: "Makefile: Add design-aligned aliases (up, mesh, verify, down, reset_*, logs)"
    status: fixed
    priority: P1
    prompt: |
      Provide thin aliases that map to existing testbed targets for parity with DESIGN.md:
      - up: `up_389ds init_389ds`
      - mesh: `repl_pod_mesh`
      - verify: `verify_389ds`
      - down: `down_389ds`
      - reset_soft: `reset_soft`
      - reset_hard: `reset_hard`
      - logs: `bundle_logs`
    files_to_edit:
      - Makefile
    acceptance_criteria:
      - `make up`, `make mesh`, `make logs` run successfully and map to the corresponding existing targets.

  - title: "Playbooks: Add provision.yml, replicate.yml, verify.yml, collect_logs.yml"
    status: fixed
    priority: P1
    prompt: |
      Create top-level playbooks aligning to the design’s flow:
      - provision.yml: Wait for containers healthy, ensure suffix, run TLS role, prep replication pre-reqs.
      - replicate.yml: Run `dirsrv_repl` using inventory/group vars for topology.
      - verify.yml: Basic checks (binds, sample search across nodes, optional test entry add).
      - collect_logs.yml: Gather DS logs from volumes and `podman logs -t <name>`; place under `.ansible/artifacts/<run>/`.
    files_to_add:
      - provision.yml
      - replicate.yml
      - verify.yml
      - collect_logs.yml
    acceptance_criteria:
      - Each playbook runs idempotently; supports `--check` where applicable.

  - title: "Tests/Vars: Replace o=example with dc=example,dc=com across repo"
    status: fixed
    priority: P0
    prompt: |
      Ensure the entire test stack references the unified suffix:
      - Update `test/compose_vars.yml` backend suffix, `test/seed.yml` DN variables, and any `ldapsearch` bases.
      - Update example LDIF (`testdata/example.ldif`) if it contains `o=example`.
    files_to_edit:
      - test/compose_vars.yml
      - test/seed.yml
      - testdata/example.ldif
    acceptance_criteria:
      - No occurrences of `o=example` in the repository (except historical docs) per ripgrep.

  - title: "Vault: Store dirsrv_password in Ansible Vault and wire through"
    status: todo
    priority: P1
    prompt: |
      Secure secrets per repo policy:
      - Create `group_vars/all/vault.yml` with `dirsrv_password: <secret>` using Ansible Vault.
      - Run plays with `--ask-vault-pass` or a vault ID.
      - Ensure roles use `dirsrv_password` (already standard) and do not log secrets (`no_log: true`).
    files_to_add:
      - group_vars/all/vault.yml (encrypted)
    commands:
      - ansible-vault create group_vars/all/vault.yml
      - ansible-playbook -i inventory.yml site.yml --ask-vault-pass --check
    acceptance_criteria:
      - Secrets not present in plaintext; runs succeed when vault is provided.

  - title: "Makefile/Net: Align NET_NAME and network bootstrap to dsnet"
    status: fixed
    priority: P2
    prompt: |
      Update Makefile variables and network create target to `dsnet` for consistency with compose:
      - Set `NET_NAME := dsnet`.
      - Ensure `net` target creates `dsnet`.
    files_to_edit:
      - Makefile
    acceptance_criteria:
      - `make net` creates or confirms `dsnet` network.

  - title: "Replication Manager: Verify creation and inbound bind DN authorization"
    status: fixed
    priority: P1
    prompt: |
      Confirm the replication role’s steps succeeded on each node:
      - `replication create-manager --name "replication manager"` exists on suppliers/consumers when SIMPLE auth is configured.
      - `replication set --repl-add-bind-dn` ran on targets matching agreements.
      - Add a verify task to query `cn=config` and ensure the user and allowed bind DNs are present; fail if missing.
    files_to_edit:
      - roles/dirsrv_repl/tasks/enable.yml (optional: add verify block guarded by a var)
      - verify.yml (add explicit checks)
    acceptance_criteria:
      - Verify play fails fast if replication manager or allowed bind DN is missing.

  - title: "Docs/Design Parity: Optional volumes under /srv/389ds-lab inside Podman VM"
    status: todo
    priority: P2
    prompt: |
      For exact design parity, mount host directories into the Podman VM at `/srv/389ds-lab` and use those as compose volume sources.
      - Configure Podman machine with a persistent mount (if not already): `podman machine set --volume=$HOME/Projects/389ds-lab:/srv/389ds-lab` then reboot VM.
      - Adjust compose bind `source:` paths to `/srv/389ds-lab/...` structure.
    files_to_edit:
      - compose/podman-compose.389ds.yml
    commands:
      - podman machine stop; podman machine set --volume "$HOME/Projects/389ds-lab:/srv/389ds-lab"; podman machine start
    acceptance_criteria:
      - DS logs/config/data visible under `/srv/389ds-lab` inside VM and persist across container restarts.

  - title: "Test inventories: Keep Podman container hostnames, but set FQDN in replication host field"
    status: fixed
    priority: P1
    prompt: |
      Avoid breaking the Podman connection plugin while moving agreements to FQDNs:
      - Leave inventory hosts as container names (e.g., `ds-s1`), with `ansible_connection: containers.podman.podman`.
      - In replication vars (`dirsrv_repl_nodes`), set `host: <FQDN>` and `protocol: LDAPS`, `port: 636`.
    files_to_edit:
      - test/inventory.compose.pod.yml
      - test/inventory.compose.pod4.yml
      - test/repl_vars.yml
      - test/repl_mesh_vars.yml
    acceptance_criteria:
      - Ansible connects via Podman; replication agreements use FQDN:636.

  - title: "Observability: Add a top-level logs alias and confirm bundle contents"
    status: fixed
    priority: P2
    prompt: |
      Add a `logs` alias to call `bundle_logs` and confirm contents include:
      - `.ansible/test_logs/podman-*.log`
      - DS logs copied from bind mounts per service
      - `.ansible/artifacts` snapshots from roles
    files_to_edit:
      - Makefile
    acceptance_criteria:
      - `make logs` creates a timestamped archive containing the above paths; archive opens without errors.
